apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: llm-stack
data:
  litellm_config.yaml: |
    model_list:
      - model_name: gpt-3.5-turbo
        litellm_params:
          model: ollama/gpt-3.5-turbo
          api_base: "http://ollama.llm-stack.svc.cluster.local:11434"
          input_cost_per_second: 0.000005
      - model_name: llama3
        litellm_params:
          model: ollama/llama3
          api_base: "http://ollama.llm-stack.svc.cluster.local:11434"
          input_cost_per_second: 0.000005
      - model_name: text-embedding-3-small
        litellm_params:
          model: ollama/nomic-embed-text
          api_base: "http://ollama.llm-stack.svc.cluster.local:11434"
      - model_name: mxbai-embed-large
        litellm_params:
          model: ollama/mxbai-embed-large
          api_base: "http://ollama.llm-stack.svc.cluster.local:11434"
    general_settings:
      master_key: os.environ/LITELLM_MASTER_KEY
      database_url: os.environ/DATABASE_URL
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litellm
  namespace: llm-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: litellm
  template:
    metadata:
      labels:
        app: litellm
    spec:
      containers:
      - name: litellm
        image: ghcr.io/berriai/litellm:main-latest
        args: ["--config", "/app/config.yaml", "--port", "4000"]
        ports:
        - containerPort: 4000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: llm-stack-secret
              key: litellm-database-url
        - name: LITELLM_MASTER_KEY
          valueFrom:
            secretKeyRef:
              name: llm-stack-secret
              key: litellm-master-key
        volumeMounts:
        - name: config-volume
          mountPath: /app/config.yaml
          subPath: litellm_config.yaml
      volumes:
      - name: config-volume
        configMap:
          name: litellm-config
---
apiVersion: v1
kind: Service
metadata:
  name: litellm
  namespace: llm-stack
spec:
  selector:
    app: litellm
  ports:
    - protocol: TCP
      port: 4000
      targetPort: 4000
